---
title:  "#1 - 기계학습과 딥러닝 이란?" 
excerpt: "텐서플로우 기초"

categories:
  -  AI
tags:
  - [AI, 기계학습, 딥러닝]

last_modified_at: 2022-12-27
---

## 기계학습이란?
- 기계학습 : 컴퓨터가 투입된 데이터를 분석하고 추론하여 명시적인 지시 없이 테스크를 수행하도로 하는 알고리즘의 일종  
  ex)
  + 스팸메일 분류 시스템 **'하드코딩'** VS **'기계학습'**
    + 하드코딩 : 스팸메일의 특징을 사람이 직접 찾고, 그 특징을 사람이 실제로 코드에 넣어서 스펨메일을 분류
    + 기계학습 : 스펨메일의 특징을 스스로 파악하고, 해당 메일이 스펨메일인지 확률에 근거하여 분류
<br>
+ 기계학습 모델 ;  데이터가 실제로 들어있는 레이어와 그 사이를 연결한 형태 (perceptron)  
  + 레이어(layer) : 사전적 의미로 Layer는 층을 의미함. 기계학습에서는 실제로 데이터나 변수가 저장되는 곳을 의미  
  + 가중치(weight) : 하나의 노드에 포함된 데이터가 연결된 다음 노드의 데이터에 얼마나 큰 영향을 미치는지 표현하는 파라미터  
  + 편향(vias) : 가중치가 관여하는 영향을 제외한, output데이터에 직접 영향을 주는 파라미터

<br>    
+ 기계학습 방법  
  + 지도학습(supervised learning) : 훈련 데이터를 분석할 때 가이드라인을 제공하여 주어진 데이터가 어떤 종류인지 분류하게 함
    + 데이터에 포함된 속성을 벡터형태로 표현하고, 각각의 벡터에 대하여 실제 데이터가 원하는 결과가 무엇인지 유추하는 회귀분석 과정을 통함
    + 알맞는 방법을 통해 기존 훈련 데이터로 부터 얻지 못한 상황까지도 분석할 수 있게 일반화된 처리가 필요(ex. 사람과 동물을 구분)
    + 학습 과정에서 각각의 입력 데이터는 라벨을 가지고, 해당 라벨을 바탕으로 학습하여 확률에 근거한 분류를 함  
  <br>
  + 비지도학습(un-supervised learning) : 지도학습고는 반대로 가이드라인을 제공하지 않고, 주어진 데이터가 어떤 종류인지 분류하게 함  
    + 데이터가 자체가 부족하거나, 데이터 수집에 너무 많은 비용이 소모될 때 사용함  
    + 데이터는 라벨을 가지고 있지 않고, 분석된 데이터에서 유사도를 직접 파악하여 확률에 근거한 분류를 한다.  
  <br>
  + 강화학습(reinforcement learning) : 어떠한 환경안에서 현재 상태와 선택가능한 행동들중에서 최적의 동작을 하게 하는 학습방법
    + 선택 가능한 행동들에 보상을 부여하여 보상을 최대화 하는 행동 또는 행동 순서를 선택하게 한다.
    + 만약 행동이 비정상 적이거나, 개발자의 의도와 맞지 않더라도 명시적인 정정이 일어나지 않는다. (즉, 이상하게 움직여도 보상이 크면 OK)
    + 게임이론, 제어이론, 정보이론, 통계학, 유전 알고리즘 분석 에서 주로 연구 된다.
    
    
  <br><br>

|지도학습|비지도학습|강화학습|
|---|---|---|
|레이블이 지정된 데이터 처리|레이블이 지정되지 않은 데이터 처리|보상의 최대를 구하는 데이터 처리|
|예측이 쉽다|예측이 어렵다|예측이 어렵고, 결과가 비정상적일 수 있다|
  
  
## 딥러닝이란?
- 기계학습의 perceptron에 숨겨진 레이어(hidden layer)가 추가된 형태의 학습 알고리즘의 일종
- 인간의 사고와 비슷하게 구현해보려는 시도에서 등장 (Neural Network)

+ hidden layer : 생각을 잠시 저장하는 공간
  + 특성추출(feature extraction) : 동일한 특징이나, 특정 알고리즘에 따라 분류하여 각각의 hidden layer node 로 넘어수

+ 정확히 예측 되었는지 평가하기 위하여 손실함수(Loss function)을 사용함
  + 컴퓨터가 연산을 반복하면서 정답과 가장 근접한 weight를 찾도록 유도한다.
  + 손실함수(Loss fucntion) : 총오차를 계산하는 수식, 일반적인 신경망 학습에는 MSE와 Cross Entropy를 사용한다.
![ML](https://user-images.githubusercontent.com/105574034/210136974-1322e6ea-2258-45de-98cd-23f375626c46.jpg)

<br>
- **MSE 와 MAE의 차이 비교**
![Loss function](https://user-images.githubusercontent.com/105574034/210136876-741c97ae-19cc-4830-8987-df108167e6f2.jpg)

|종류|장점|단점|
|---|---|---|
|Mean Squared Error| - 실제 정받에 대한 정답률의 오차뿐만 아니라 다른 오답에 대한 정답률의 오차도 함께 계산한다. <br> - MAE와 달리 최적값에 가까워질수록 이동값이 다르게 변화하기 때문에 최적값에 수렴하기 용의하다.| - 값을 제곱하기 때문에 절댓값이 1미만인 겂은 더 작아지고, 1보다 큰 값은 더 커지는 왜곡이 발생할 수 있다. <br> <br> - 제곱하기 때문에 특이값의 영향을 많이 받는다.|
|Mean Absolute Error| - 전체 데이터의 학습된 정도를 쉽게 파악할 수 있다.| - 어떤 식으로 오차가 발생했거나 음수인지 양수인지 알 수 없다. <br> - 최저점에서 연속적이지 못하다. 따라서 최저값에 수렴하기가 어렵다.|

<br>
+ Entropy
  + 엔트로피는 정보량의 기댓값을 나타내는 척도라고 할 수 있다. (확률 분포의 무질서도 or 불확실성 등)
  + 정보가 항상 일정한 데이터를 가지고 있다가도, 특별한 데이터가 등장하면서 예측했던 데이터와 크게 다를때 엔트로피가 크다고 할 수 있다.
  + 예시로 카페에서 항상 아메리카노를 먹는 손님이 어느날 갑자기 카페라떼를 시키는 것과 같이 예상하기 어려운 상황을 엔트로피가 크다고 한다.
  <br>
+ Cross Entropy
  + Entropy와는 다르게 두가지 확률 분포가 얼마나 비슷한지를 수리적으로 나타내는 개념이다.
  + (필자는 초기에 Cross Entropy가 확률(probability)학에서 사용되는 Co-Variance와 유사하다고 생각했다.)
  + Cross Entropy에서는 실제값과 예측값이 맞는 경우에는 0으로 수렴하고 값이 틀릴경우에는 값이 커진다. 따라서 두 확률 분포가 얼마나 다른지 알수 있다.
  <br>
+ Binary Cross Entropy
  + 이진 분류(ex. 맞다 vs 아니다, 같다 vs 다르다, 양성 vs 음성 등)훈련을 해야 할때 사용하면 유리하다.
  + 예측값은 0과 1사이의 값으로 나오며 0또는 1에 가까울수록 둘중 하나의 클래스에 가깝다.
  <br>
+ Categorical Crossentropy
  + 분류해야될 클래스가 3개 이상일때 사용된다. (ex. 사람 vs 고양이 vs 개 등)
  + 라벨을 one-hot encoding 형태로 제동될 때 사용가능하다.
  + softmax function과 함께 쓰이는 일이 많다.
  <br>
+ Sparse Categorical Crossentropy
  + Catergorical Crossentropy와 유사하지만 라벨이 0, 1, 2 와 같이 정수로 제공될 때 사용한다. (ex. 사람 = 0 vs 고양이 = 1 vs 개 = 2 등)


<br>
 - hidden layer가 "뇌처럼 생각하는 공간" 역할을 잘 수행하기 위해서는 활성함수를 사용해야한다. 이는 다음 포스트에서 다루겠다.





<br>

    😺 오타나 논리적 오류 지적은 언제든지 환영합니다!😺   
    항상 읽어주셔서 감사합니다~ 🙏
