---
title:  "#3 - 가중치" 
excerpt: "텐서플로우 기초"

categories:
  -  AI
tags:
  - [AI, 기계학습, 딥러닝]

last_modified_at: 2023-01-08
---

## 오차를 최소화 하는 최적의 가중치 찾기
 - 초기 입력된 가중치에서 값이 변할수록 오차도 함께 
 - 가중치가 하나뿐이라면 2차원 그래프로 판단 가능하지만, 실제로 가중치는 매우 많음
 - 따라서 적절한 가중치의 변화를 주기위해 경사하강법(Gradient descent)을 사용함

## 경사하강법(Gradient descent)
 - 가중치를 증가시킬지, 감소시킬지 판단하는 기준
 - 경사를 타고 하강하는 형태
 - 현재 가중치에서 접선의 기울기를 뺀 값이 다음 가중치
 - 계속해서 다음 가중치를 찾아가면서 가장 손실이 적은 가중치를 찾는다.
 ![weight](https://user-images.githubusercontent.com/105574034/211187730-5dbcb21c-6de4-48ce-b177-1735cd3f2068.jpg)


## 딥러닝 학습과정
1. 가중치값을 랜덤하게 하나 가진다.
2. 가중치값의 변화를 이용하여 총 손실 오차를 계산한다.
3. 경사하강법을 사용하여 새로운 가중치값을 가진다.
4. 2.와 3.의 과정을 반복하여 손실 오차가 가장 적은 가중치를 찾아낸다.

## Local minima & Learning rate
 - 가중치 변화 그래프에서 기울기가 0이 되는 구간이 여러개 존재함(local minima)
 - 실제 최저점을 구하기 위해서 새로운 해결책이 필요함
 - 경사하강법에서 기울기만 빼는 것이 아닌, Learning rate와 기울기를 곱한 값을 빼기
 - Learning rate : 임의로 정한 alpha값, local minima를 건너띄기하게 해준다.
 - Learning rate에는 정답이 없음, 따라서 trial and error과정을 통해 가장 적절한 Learning rate를 찾아야함

## Learning rate optimizer
 + 최적의 Learning rate를 가변적으로 최적화 해주는 알고리즘
 + 여러가지 종류의 알고리즘을 사용해보고 최적의 알고리즘을 선택해야함
 + Momentum
    + 가속도를 유지
 + AdaGrad
    + 자주변하는 가중치에는 Learning rate가 작게
    + 자주변하지 않는 가중치에는 Learning rate가 크게
 + RMSProp
    + AdaGrad방식과 유사하나 제곱된 형태
 + AdaDelta
    + AdaGrad에서 Learning rate가 너무 작아져서 학습이 되지 않는 현상을 방지함
 + Adam
    + RMSProp + Momentum 방식
    + 가장 많이 쓰인다


- 이후에 역전파 알고리즘을 사용한 가중치 업데이트 법에 대하여 추가할 예정입니다.


<br>

    😺 오타나 논리적 오류 지적은 언제든지 환영합니다!😺   
    항상 읽어주셔서 감사합니다~ 🙏